# 实时分析淘宝用户的行为数据

## 1 环境准备

### 1.1 组件准备

处理实时数据，选择以下组件进行处理，此外主要以Flink为主，并且使用Flink SQL来进行流处理。以此连接外部数据源（如Kafka、Elasticsearch、MySQL），并实现实时报表的指标统计。

> 选择Flink SQL的理由如下：
>
> 1. **SQL语法支持：** Flink SQL允许使用SQL语句进行流处理和批处理，这使得比较熟悉SQL的我们能够更轻松地使用Flink进行大数据处理。
>
> 2. **统一的批处理和流处理：** Flink SQL提供了统一的API，使得我们可以在同一个虚拟机上运行批处理和流处理任务，从而简化了开发和维护工作。
>
> 3. **事件时间处理：** Flink具有强大的事件时间处理功能，可以处理乱序事件，并提供了窗口操作来处理基于事件时间的数据。
>
> 4. **状态管理：** Flink SQL支持状态管理，这对于处理有状态的流处理任务非常关键。它可以帮助您跟踪和维护处理过程中的状态信息。
>
> 5. **灵活性和可扩展性：** Flink是一个高度可扩展和灵活的系统，可以适应不同的业务需求。您可以使用Flink SQL轻松构建复杂的数据处理流程。
>
>    此外我们的团队中有不熟悉Java、Spring Boot开发的成员，故Flink SQL是我们团队最优选择

|     组件      |                             版本                             |
| :-----------: | :----------------------------------------------------------: |
|     Flink     | [1.172](https://www.apache.org/dyn/closer.lua/flink/flink-1.17.2/flink-1.17.2-bin-scala_2.12.tgz) |
|   ZooKeeper   | [3.5.7](https://archive.apache.org/dist/zookeeper/zookeeper-3.5.7/) |
|     Kafka     |         [2.5.0](https://kafka.apache.org/downloads)          |
| Elasticsearch | [7.6.0](https://www.elastic.co/cn/downloads/past-releases/elasticsearch-7-6-0) |
|    Kibana     | [7.6.0](https://www.elastic.co/cn/downloads/past-releases/kibana-7-6-0) |

> 其他组件作用：
> 1.Kafka：Kafka用于存储用户行为数据，作为Flink SQL的数据源。通过Kafka，我们能够以可扩展和可靠的方式处理大规模的实时数据流。
>
> 2.ZooKeeper：Zookeeper被用于配置Kafka，特别是连接Kafka的参数。
>
> 3.Elasticsearch：Elasticsearch用于存储实时报表的结果，包括每小时成交量、每10分钟累计独立用户数和商品类目销量排行。
>
> 4.Kibana：Kibana用于从Elasticsearch索引中检索数据，以实现诸如可视化等功能。通过Kibana，我们创建仪表板，汇总多个可视化图表，展示各种指标，将大数据模式简化。

#### 1.1.1 Flink Connector 准备

由于我们使用Flink与其他组件进行连接，需要一些Connector进行帮助，故在[Flink开发文档](https://nightlies.apache.org/flink/flink-docs-release-1.17/release-notes/flink-1.17/)中找到兼容的Connector进行下载。

> 下载之后需要将jar包放入Flink安装目录下的/lib目录中。

|     组件      |                        jar                        |                             版本                             |
| :-----------: | :-----------------------------------------------: | :----------------------------------------------------------: |
|     Kafka     |       flink-sql-connector-kafka-1.17.2.jar        | [universal](https://repo.maven.apache.org/maven2/org/apache/flink/flink-sql-connector-kafka/1.17.2/flink-sql-connector-kafka-1.17.2.jar) |
| Elasticsearch | flink-sql-connector-elasticsearch7-3.0.1-1.17.jar | [flink-connector-elasticsearch7](https://repo.maven.apache.org/maven2/org/apache/flink/flink-sql-connector-elasticsearch7/3.0.1-1.17/flink-sql-connector-elasticsearch7-3.0.1-1.17.jar) |



### 1.2 其他外部准备

#### 1.2.1 外部版本

由于版本兼容性、可用性以及稳定性的要求，我们选择更换相关的其他外部版本。

|  组件   |                             版本                             |
| :-----: | :----------------------------------------------------------: |
|  Java   | [11.0.16.1](https://www.oracle.com/java/technologies/javase/jdk11-archive-downloads.html) |
|  MySQL  |     [8.0.35](https://dev.mysql.com/downloads/repo/yum/)      |
| Firefox |                           120.0.1                            |
|  bzip2  | [1.0.6](https://sourceforge.net/projects/bzip2/files/latest/download) |
| Python3 |                            3.6.8                             |

> 1.由于使用Flink Web UI时发现虚拟机已有的Chrome浏览器版本无法支持呈现，故使用终端下载Firefox浏览器以运行项目。
>
> 2.下载Firefox之后需要解压，但发现虚拟机自带的bzip无法正常解压，故下载bzip2进行解压下载。
>
> 3.数据生成器需要使用python3版本，故在终端进行升级。

#### 1.2.2 外部Flink Connector

| 组件  |                             jar                              |                             版本                             |
| :---: | :----------------------------------------------------------: | :----------------------------------------------------------: |
| MySQL | mysql-connector-j-8.0.33<br />flink-connector-jdbc-3.1.0-1.17 | [8.0.33](https://mvnrepository.com/artifact/com.mysql/mysql-connector-j/8.0.33)<br />[3.1.0-1.17](https://mvnrepository.com/artifact/org.apache.flink/flink-connector-jdbc/3.1.0-1.17) |



### 1.3 数据来源

数据来源：[淘宝用户购物行为数据集](https://tianchi.aliyun.com/dataset/649)（阿里云天池）

> 个人网盘：链接：https://pan.baidu.com/s/1Vj295MRA56YLmbCOgoesaQ 
> 提取码：2309 
> --来自百度网盘超级会员V4的分享

商品类目纬度数据来源请查看category.sql文件。

数据生成器：datagen.py

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-

# @Time : 2023/12/02 01:14
# @Author : lmh
# @Site : nnnnghj
# @Describe: 这个脚本用于读取用户行为数据，并将其发送到Kafka主题

import sys
import time
import json
import queue
from kafka import KafkaProducer
from concurrent.futures import ThreadPoolExecutor

# Kafka服务器地址，在此项目中只需要在虚拟机运行即localhost
servers = ['localhost:9092', ]

# Kafka主题
topic = 'user_behavior'

# 用户行为日志文件的路径
path = '/root/UserBehaviorFromTaobao_Stream/user_behavior.log'

# 初始化Kafka生产者，配置服务器和值序列化方式
producer = KafkaProducer(bootstrap_servers=servers, value_serializer=lambda m: json.dumps(m).encode('utf-8'))

def send(line):
  # 解析每一行数据
  cols = line.strip('\n').split(',')
  # 转换时间戳为可读格式
  ts = time.strftime("%Y-%m-%dT%H:%M:%SZ", time.localtime(int(cols[4])))
  # 构造要发送的消息
  value = {"user_id": cols[0], "item_id": cols[1], "category_id": cols[2], "behavior": cols[3], "ts": ts}
  # 发送消息到Kafka
  producer.send(topic=topic, value=value).get(timeout=10)

if __name__ == "__main__":
  # 默认线程数（性能优化、CPU太低了差不多这个线程数、以前做过的项目差不多就是这个值舒服一点）
  num = 2000

  # 如果命令行参数存在，使用命令行指定的线程数
  if len(sys.argv) > 1:
    num = int(sys.argv[1])

  # 定义一个有界线程池（控制一下并发度，别给终端爆了）
  class BoundThreadPoolExecutor(ThreadPoolExecutor):
    def __init__(self, *args, **kwargs):
      super(BoundThreadPoolExecutor, self).__init__(*args, **kwargs)
      self._work_queue = queue.Queue(num * 2)

  # 读取用户行为日志文件
  with open(path, 'r', encoding='utf-8') as f:
    # 使用线程池处理每一行数据
    pool = BoundThreadPoolExecutor(max_workers=num)
    for arg in f:
      pool.submit(send, arg)
    # 等待所有任务完成
    pool.shutdown(wait=True)
```



## 2 Flink SQL客户端实现

```shell
#在Flink安装目录下终端启动Flink SQL
./bin/sql-client.sh embedded -l lib --指定/lib目录下的jar包运行
```

### 2.1 Kafka数据表

#### 2.1.1 建表以存放数据源

数据通过Kafka连接器从指定的主题中读取，并且假定数据以JSON格式存储。Flink SQL表使用事件时间处理，并设置了一个水印策略来处理可能存在的数据延迟。

创建一个名为 user_behavior 的表。

> 此外，代码还包含了处理Kafka连接和JSON数据格式的一些配置选项。

```sql
CREATE TABLE user_behavior (
    -- 定义表的字段
    user_id BIGINT,                  -- 用户ID
    item_id BIGINT,                  -- 商品ID
    category_id BIGINT,              -- 商品类别ID
    behavior STRING,                 -- 用户行为（如点击、购买等）
    ts TIMESTAMP(3),                 -- 时间戳，精确到毫秒，数据源的精度到毫秒
    proctime AS PROCTIME(),          -- Flink处理时间属性
    WATERMARK FOR ts AS ts - INTERVAL '5' SECOND  -- 设置水印，用于事件时间处理。水印延迟5秒
) WITH (
    'connector' = 'kafka',                  -- 数据源连接器类型：Kafka
    'topic' = 'user_behavior',              -- Kafka主题名称
    'properties.bootstrap.servers' = 'localhost:9092',  -- Kafka服务器地址
    'properties.group.id' = 'testGroup',    -- Kafka消费者组ID
    'properties.zookeeper.connect' = 'localhost:2181',     -- Zookeeper 服务器地址
    'format' = 'json',                      -- 数据格式：JSON
    'scan.startup.mode' = 'earliest-offset',  -- 读取策略，从最早的偏移量开始读取
    'json.fail-on-missing-field' = 'false',  -- 设置为false表示JSON中的缺失字段不会导致失败
    'json.ignore-parse-errors' = 'true'     -- 忽略JSON解析错误
);
```

> 在Flink中，水印（Watermark）是一种用于处理事件时间（event time）中的延迟数据的机制。在流处理系统中，由于网络延迟、系统故障或其他原因，数据可能不会按照其生成的时间顺序到达。水印提供了一种方法来指示特定时间点之前的数据都已到达，这对于正确处理时间窗口非常重要。
>
> 在我的Flink SQL代码中，`WATERMARK FOR ts AS ts - INTERVAL '5' SECOND` 表示：
>
> - **水印设置在`ts`字段上**：这意味着`ts`字段代表事件的时间戳，Flink将使用这个字段来理解数据的时间顺序。
> - **水印延迟5秒**：系统将当前观察到的最大事件时间戳减去5秒作为水印。这意味着Flink将等待5秒以接收可能晚到的事件。如果`ts`是某个事件的时间戳，那么系统会认为所有时间戳小于`ts - 5秒`的事件都已经到达。

> 这种设置的作用包括：
>
> 1. **处理乱序数据**：允许系统处理不完全按照事件时间顺序到达的数据。
> 2. **触发时间窗口的计算**：水印通常用于确定何时可以安全地计算和关闭时间窗口。在本项目中防止数据处理崩溃。

#### 2.1.2 查看是否成功建立表格

```sql
-- 查看是否成功建立表格
SELECT * FROM user_behavior;
```



### 2.2 每小时成交量

#### 2.2.1 建表以存放每小时成交量

创建一个名为 buy_cnt_per_hour 的表，用于存储每小时购买行为的计数。

```sql
CREATE TABLE goum (
    hour_of_day BIGINT,    -- 小时数，代表一天中的小时
    buy_cnt BIGINT,        -- 购买计数，表示在该小时内的购买行为数量
    event_time TIMESTAMP(3),  -- 新增时间戳字段，代表事件时间
    WATERMARK FOR event_time AS event_time - INTERVAL '5' SECOND  -- 根据 event_time 字段设置水印
) WITH (
    'connector' = 'elasticsearch-7',  -- 使用Elasticsearch 7作为连接器
    'hosts' = 'http://localhost:9200',  -- Elasticsearch服务器地址
    'index' = 'buy_cnt_per_hour',       -- Elasticsearch中的索引名
    'format' = 'json'                   -- 数据格式为JSON
);
```

#### 2.2.2查询每小时的成交量并插入

将从 user_behavior 表中计算得到的每小时购买行为的计数插入到 buy_cnt_per_hour 表。

```sql
INSERT INTO buy_cnt_per_hour
SELECT 
    HOUR(TUMBLE_START(ts, INTERVAL '1' HOUR)) AS hour_of_day, -- 从时间戳字段 ts 计算得到的小时数，使用滚动窗口（TUMBLE）函数，窗口大小为1小时
    COUNT(*) AS buy_cnt,  -- 计算在每个窗口内的行为 'buy' 的事件数量
    TUMBLE_START(ts, INTERVAL '1' HOUR) AS event_time  
FROM 
    user_behavior  -- 来源表
WHERE 
    behavior = 'buy'  -- 只考虑行为为 'buy' 的事件
GROUP BY 
    TUMBLE(ts, INTERVAL '1' HOUR);  -- 按每小时的滚动窗口分组，窗口基于时间戳字段 ts
```



### 2.3每十分钟累计独立用户数

#### 2.3.1建表以存放每十分钟累计独立用户数

创建一个名为 cumulative_uv 的表，用于存储每10分钟的累积独立用户数（UV）。

```sql
CREATE TABLE cumulative_uv (
    time_str STRING,       -- 时间字符串，表示每10分钟的结束时间
    uv BIGINT              -- 独立用户数（UV）
) WITH (
    'connector' = 'elasticsearch-7',   -- 使用Elasticsearch 7作为连接器
    'hosts' = 'http://localhost:9200',  -- Elasticsearch服务器地址
    'index' = 'cumulative_uv',          -- Elasticsearch中的索引名
    'format' = 'json'                   -- 数据格式为JSON
);
```

#### 2.3.2 查询每10分钟内的独立用户数

创建一个视图 uv_per_10min，用于计算每10分钟内的独立用户数。

```sql
CREATE VIEW uv_per_10min AS
SELECT
    -- 使用TUMBLE_END函数来获取每个10分钟窗口的结束时间，并格式化为字符串
     DATE_FORMAT(TUMBLE_END(ts, INTERVAL '10' MINUTE), 'yyyy-MM-dd HH:mm:ss') as time_str,
    -- 计算每个窗口内不同用户的数量（即UV）
    COUNT(DISTINCT user_id) as uv
FROM 
    user_behavior -- 来源表
GROUP BY 
    TUMBLE(ts, INTERVAL '10' MINUTE); -- 按每10分钟的滚动窗口分组，窗口基于时间戳字段 ts
```

#### 2.3.3 查询每10分钟内的独立用户数并插入

将从 uv_per_10min 视图中计算得到的每10分钟内的独立用户数插入到 cumulative_uv 表。

```sql
INSERT INTO cumulative_uv
SELECT time_str, uv
FROM uv_per_10min;
```



### 2.4 商品类目销量排行

#### 2.4.1 存储商品类目信息

创建一个名为 category_dim 的表，用于存储商品类目信息.

```sql
CREATE TABLE category_dim (
    sub_category_id BIGINT,             -- 子类目ID
    parent_category_name STRING         -- 父类目名称
) WITH (
    'connector' = 'jdbc',                           -- 使用JDBC连接器
    'url' = 'jdbc:mysql://localhost:3306/flink?serverTimezone=UTC', -- MySQL数据库的URL，包括时区信息
    'table-name' = 'category',                      -- 数据库中的表名
    'driver' = 'com.mysql.cj.jdbc.Driver',          -- MySQL驱动类，适用于MySQL 8
    'username' = 'root',                            -- 数据库用户名
    'password' = 'syclmh102309',                            -- 数据库密码
    'lookup.cache.max-rows' = '5000',               -- 查找缓存的最大行数
    'lookup.cache.ttl' = '10min'                    -- 查找缓存的生存时间
);
```

#### 2.4.2 存储每个商品类目的购买计数

创建一个名为 top_category 的表，用于存储每个商品类目的购买计数。

```sql
CREATE TABLE top_category (
    category_name STRING,  -- 类目名称
    buy_cnt BIGINT         -- 购买计数
) WITH (
    'connector' = 'elasticsearch-7',       -- 使用Elasticsearch 7作为连接器
    'hosts' = 'http://localhost:9200',  -- Elasticsearch服务器地址
    'index' = 'top_category',              -- Elasticsearch中的索引名
    'sink.bulk-flush.max-actions' = '1',   -- 设置批量写入的操作次数
    'format' = 'json'                      -- 数据格式为JSON
);
```

#### 2.4.3 数据丰富化

创建一个视图 rich_user_behavior，将用户行为数据丰富化，加入类目名称。

```sql
CREATE VIEW rich_user_behavior AS
SELECT 
    U.user_id, U.item_id, U.behavior, 
    C.parent_category_name as category_name  -- 关联类目名称
FROM 
    user_behavior AS U
LEFT JOIN 
    category_dim FOR SYSTEM_TIME AS OF U.proctime AS C  -- 时态表连接，以系统时间为基准
ON 
    U.category_id = C.sub_category_id;  -- 连接条件
```

#### 2.4.4 统计得到的每个类目的购买计数并插入

将从 rich_user_behavior 视图中计算得到的每个类目的购买计数插入到 top_category 表。

```sql
INSERT INTO top_category
SELECT 
    category_name, 
    COUNT(*) AS buy_cnt  -- 统计每个类目的购买次数
FROM 
    rich_user_behavior
WHERE 
    behavior = 'buy'  -- 只考虑购买行为
GROUP BY 
    category_name;    -- 按类目名称分组
```



## 3 Kibana创建

### 3.1 建立索引

#### 3.1.1 打开Firefox浏览器

由于需要使用Firefox浏览器进行操作，首先在终端打开它。

```shell
firefox --打开浏览器
```

#### 3.1.2  打开Kibana

![image-20231217175431910](https://s2.loli.net/2023/12/22/RtzOEUuNA2bIgTP.png)

#### 3.1.3 设置索引

![image-20231217175527844](https://s2.loli.net/2023/12/22/u89yealbNtjPW7X.png)

![image-20231217175549798](https://s2.loli.net/2023/12/22/cs5RO29q68Mxi3J.png)

![image-20231217175615508](https://s2.loli.net/2023/12/22/AJnhetTXbjCOGDM.png)

![](https://s2.loli.net/2023/12/22/ZpWOYkvlKmRMszG.png)

![image-20231217175827613](https://s2.loli.net/2023/12/22/tmEFgr23aTS7P5Y.png)

成功：

![image-20231217175846804](https://s2.loli.net/2023/12/22/pCHJYkuVx2amZrw.png)



#### 3.1.4 建立可视化

##### 3.1.4.1 以“每小时成交量图表“柱状图为例

![image-20231217180205253](https://s2.loli.net/2023/12/22/4AYwCa7S8pfVkTD.png)

![image-20231217180221774](https://s2.loli.net/2023/12/22/ts3627VlrIpj8SR.png)

![image-20231217180418772](https://s2.loli.net/2023/12/22/ICR5LWejP8ScUrV.png)

![image-20231217180435731](https://s2.loli.net/2023/12/22/KqOT43QZ1xisIob.png)

![image-20231220161812760](https://s2.loli.net/2023/12/22/bANt2jr8oqki1I3.png)

![image-20231220161854306](https://s2.loli.net/2023/12/22/Ur6qzXlWbgvMs8e.png)

即可获得需要的图表。



## 4 最终效果与实践感悟

### 4.1 最终效果

#### 4.1.1 购买行为随时间变化图表

![image-20231220205109764](https://s2.loli.net/2023/12/22/GC5NfD7v48KeqbT.png)

#### 4.1.2 每个类目的总销量

![image-20231220205335674](https://s2.loli.net/2023/12/22/SWEN3Cf9sIehwxX.png)

#### 4.1.3 最大或总独立用户数

![image-20231220205617124](https://s2.loli.net/2023/12/22/I4XVKqQ7jrBaCMl.png)

#### 4.1.4 最大成交量和平均成交量

![image-20231220205704480](https://s2.loli.net/2023/12/22/MjXFt7w69uyCRpz.png)

#### 4.1.5 小时购买与独立用户数对比图表

![image-20231220205807118](https://s2.loli.net/2023/12/22/OMdiGv6XSuD9tRV.png)

#### 4.1.6 销量热力图

![image-20231220211007525](https://s2.loli.net/2023/12/22/U4mTgBhcvqrj8O2.png)

#### 4.1.7 销量与独立用户数比较

![image-20231220215403722](https://s2.loli.net/2023/12/22/ME91W6cdbCPQ4HU.png)

#### 4.1.8 独立用户数

![image-20231220215715876](https://s2.loli.net/2023/12/22/8YvimdlysGFgc34.png)

#### 4.1.9 集合展示

![image-20231220225346330](https://s2.loli.net/2023/12/22/lLm9nqiFINEXrOz.png)

### 4.2 技术问题和挑战汇总

#### 4.2.1 文件上传和处理问题

在过程中我发现当下版本的一些大数据组件UI仍热存在问题：用户界面提示不明确、反馈延迟、缺乏清晰的操作指南等，对于我们学习HCI的我仍热具有研究性和拓展性，我阅读HCI相关论文后发现可以从以下几点出发研究：即时和明确的反馈、用户指导和流程管理、适应不同用户和情境以及用户测试和反馈循环。

#### 4.2.2 Kafka服务启动和配置问题

在实验中发现kafka压缩安装后有些时候因为系统或虚拟机的设置，使用bin启动并不能完全将kafka跑起来，所以查阅kafka社区后发现可以集成system中进行启动，将其完全启动。正确配置和启动关键服务是确保数据流稳定性的基础。

#### 4.2.3 数据源验证和数据写入问题

在验证 Kafka 数据源和确认数据是否正确写入 Elasticsearch发现了问题，之后查阅有关资料以及知道了例如GET等kibana、es的操作语句。持续验证数据源和写入过程对于确保数据完整性和准确性至关重要。

#### 4.2.4 时间序列分析问题

在 Kibana 使用 Timelion 进行时间序列分析时，遇到了展示为直线图的问题。这可能是由于数据点不足或不当的数据聚合方式造成的。针对这个问题，我还需要深入理解时间序列数据的处理和可视化技巧，确保数据的充分性和适当的聚合策略。

#### 4.2.5 聚合类型选择问题

在 Kibana 图表创建过程中，选择合适的聚合类型尤其在处理日期和时间字段时面临挑战。正确选择聚合类型对于确保数据分析的准确性和可视化的有效性至关重要。因此，需要对不同聚合类型及其在各种数据场景中的应用有深入理解。

#### 4.2.6 可视化类型匹配问题

选择最合适的可视化类型来展示特定数据是一项挑战，需要对 Kibana 的可视化工具有充分的理解。这要求从数据的特性出发，选择能够最有效表达数据含义的可视化类型，以及理解不同图表类型对数据的展示方式和限制。

#### 4.2.7 数据字段准确性问题

在 Elasticsearch 查询中，确保使用的字段名称与实际字段名称的匹配性是成功创建图表的关键。这涉及到对 Elasticsearch 中数据模型的理解以及确保查询语句正确引用了这些字段。

#### 4.2.8 图表创建错误

在创建图表过程中，可能会遇到各种错误提示，如“parsing_exception”。这类问题通常要求对查询语法和数据结构有深刻的理解，以及能够准确地诊断和解决这些问题。

#### 4.2.9 时间戳格式不匹配

Flink 和 Kafka 之间处理时间戳时遇到了格式不匹配的问题。这指出了在集成不同数据处理系统时，保持数据类型和格式的一致性的重要性。不匹配的格式会导致数据流程的失败，因此需要仔细检查和调整时间戳的格式。此问题最终还是经过手动创建索引解决了。

#### 4.2.10 Elasticsearch 字段映射错误

Elasticsearch 索引中字段映射错误（如将日期字段错误地映射为字符串）会影响基于时间的查询和聚合。这突出了正确数据映射在数据索引创建和管理中的重要性，特别是对于时间敏感的数据处理。

#### 4.2.11 Flink SQL 表定义问题

在定义 Flink SQL 表时，包含了一些不支持的选项，如错误的时间戳格式设置，这表明了遵循正确的表定义语法和参数的重要性。了解并适用正确的 Flink SQL 语法和配置对于确保数据流的顺利处理至关重要。

#### 4.2.12 Kafka Broker 状态问题

确认 Kafka Broker 的正常运行状态是确保数据流稳定的重要环节。这要求对 Kafka 系统的监控和管理有深入的了解，以及能够有效地诊断和解决运行中的问题。

#### 4.2.13 Elasticsearch 连接和索引创建问题

在尝试创建 Elasticsearch 索引时，遇到了字段类型定义不准确的问题。这强调了在数据索引创建过程中对字段类型和数据结构的正确理解和配置的重要性。

#### 4.2.14 Flink 任务监控和日志分析问题

监控 Flink 作业的性能和状态以及分析日志文件是诊断问题的关键部分。这要求对 Flink 的运行机制和日志系统有深入理解，以及能够有效利用日志信息来诊断和解决问题。

#### 4.2.15 Flink SQL 语句和配置问题

Flink SQL 语句和配置中的错误导致任务失败，强调了对 Flink SQL 的精确理解以及配置的正确性对于保证数据处理的有效性的重要性。

#### 4.2.16 Kibana 索引模式和可视化问题

在 Kibana 中创建索引模式和配置可视化时遇到的问题指出了对 Kibana 可视化工具和 Elasticsearch 数据模型的深入理解的重要性，以及如何有效地利用这些工具来创建有意义的数据可视化。



## 5 代码复现指南

### 5.1 组件启动

> 需要按照ZooKeeper > Kafka > ElasticSearch > Kibana >Flink > MySQL文件启动（首次执行） > Flink SQL > datagem.py（数据生成处理代码）。

#### 5.1.1 ZooKeeper启动：

```shell
cd /opt/zookeeper/apaache-zookeeper-3.5.7-bin
./bin/zkServer.sh start
```

> jps指令查询是否正确，已经启动QuorumPeerMain即代表启动成功。

#### 5.1.2 Kafka启动

```shell
systemctl start kafka
```

> 我已经将Kafka打包到system启动文档下，可以直接启动。查询Kafka是否启动可以使用两种方式：
> 1、system查询
>
> ```shell
> systemctl status kafka
> ```
>
> 使用system查询，若成功启动会显示**active**显示；若失败则会显示failed或其他。
>
> 2、Jps查询
>
> ```shell
> jps
> ```
>
> 查询之后显示Kafka已在队列中。
>
> Ps：建议两种查询一起使用。

#### 5.1.3 Elasticsearch启动

> 我已经设置好用户es。

```shell
cd /opt/es/elasticsearch-7.6.0
su es
./bin/elasticsearch
```

> 等待一段时间后，查询方法：
> 1、Jps查询
>
> ```shell
> jps
> ```
>
> Elasticsearch已在队列中。
>
> 2、网页查询
>
> ```shell
> curl http://localhost:9200
> ```
>
> 出现内容即代表成功启动。

#### 5.1.4 Kibana启动

```shell
cd /opt/kibana/kibana-7.6.0-linux-x86_64
./bin/kibana
```

> 等待一段时间后出现并访问以下网址：
>
> ```
> http://localhost:5601
> ```
>
> 开始加载Kibana页面即代表启动成功。

> Ps：已启动的Elasticsearch与Kibana终端不允许关闭！

#### 5.1.5 Flink启动

```shell
cd /opt/flink/flink-1.17.2
./bin/start-cluster.sh
```

> 等待一段时间后，查询方法：
> 1、Jps查询
>
> ```shell
> jps
> ```
>
> StandloneSessionClusterEntrypoint和TaskManagerRunner已在队列中。
>
> 2、使用Firefox访问网页
>
> Firefox打开方式：
>
> ```shell
> firefox
> ```
>
> 输入以下网址：
>
> ```
> http://localhost:8081
> ```
>
> 显示页面即代表访问成功

#### 5.1.6 MySQL文档（首次启动）

> 因为SQL代码中需要调动MySQL以存储另一种数据类型，所以需要创建SQL表格保存数据。

```shell
mysql -uroot -psyclmh102309 /root/UserBehaviorFromTaobao_Stream/category.sql
```

#### 5.1.7 Flink SQL启动方式

```shell
cd /optcd /opt/flink/flink-1.17.2
./bin/sql-client.sh embedded -l lib
```

> SQL客户端出现即表示成功。

#### 5.1.8 datagem.py（数据生成处理代码）

```shell
python3 /root/UserBehaviorFromTaobao_Stream/datagen.py
```

### 5.2 Flink SQL代码复现

根据开发文档一段一段来就行了。